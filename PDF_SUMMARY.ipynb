{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORbwnymKJ+MgNDOu8jWPop",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshith-munakala/PDF-_SUMMARY/blob/main/PDF_SUMMARY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk-zci3ZmRZT",
        "outputId": "bb7c5daf-5d57-44dc-c447-4454384efd06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Resume-JD Skill Match Report\n",
            "========================================\n",
            "âœ… Matched Skills: {'python', 'solutions', 'pvt', 'databases', 'developer', 'github', 'strong', 'developers', 'development', 'basic', 'using', 'code', 'communication', 'like', 'data', 'skills', 'applications'}\n",
            "âŒ Missing Skills: {'django', 'india', 'mysql', 'relational', 'written', 'implement', 'hyderabad', 'aws', 'elements', 'design', 'git', 'docker', 'scalability', 'scalable', 'working', 'qualifications', 'flask', 'cloud', 'use', 'company', 'responsibilities', 'good', 'apis', 'backend', 'deployments', 'responsible', 'gcp', 'integrate', 'speed', 'proficiency', 'roles', 'knowledge', 'security', 'platforms', 'description', 'ensure', 'postgresql', 'interchange', 'excellent', 'familiarity', 'experience', 'location', 'users', 'restful', 'job', 'required', 'verbal', 'reusable', 'understanding', 'looking', 'title', 'agile', 'collaborate', 'future', 'optimize', 'technova', 'web', 'ltd', 'preferred', 'build', 'libraries', 'maximum', 'processes', 'protection', 'server', 'managing'}\n",
            "ğŸ“Š Match Percentage: 20.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)  # Initialize PDF reader\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()  # Extract text from each page\n",
        "        return text\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Convert text to lowercase and tokenize into words\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    # Keep only alphabetic tokens and remove stopwords\n",
        "    cleaned = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return set(cleaned)  # Convert list to set for comparison\n",
        "\n",
        "# Load and extract text from resume and job description PDFs\n",
        "resume_text = extract_text_from_pdf('//22981A42B0_HARSHITH MUNAKALA (1) (2).pdf')\n",
        "jd_text = extract_text_from_pdf('//JD.pdf')\n",
        "\n",
        "# Clean and extract keywords from both documents\n",
        "resume_words = clean_text(resume_text)\n",
        "jd_words = clean_text(jd_text)\n",
        "\n",
        "# Compare keywords to find matched and missing skills\n",
        "matched_skills = resume_words & jd_words  # Set intersection: common skills\n",
        "missing_skills = jd_words - resume_words  # Set difference: skills required but not in resume\n",
        "\n",
        "# Calculate the match percentage\n",
        "match_percentage = round(len(matched_skills) / len(jd_words) * 100, 2) if jd_words else 0\n",
        "\n",
        "# Print the final skill match report\n",
        "print(\"ğŸ“„ Resume-JD Skill Match Report\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"âœ… Matched Skills: {matched_skills}\")\n",
        "print(f\"âŒ Missing Skills: {missing_skills}\")\n",
        "print(f\"ğŸ“Š Match Percentage: {match_percentage}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text\n",
        "    return text\n",
        "\n",
        "# Function to summarize text using TF-IDF + cosine similarity\n",
        "def summarize_text(text, num_sentences=5):\n",
        "    sentences = sent_tokenize(text)\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return text\n",
        "\n",
        "    # Create TF-IDF matrix\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Compute similarity scores with the first sentence\n",
        "    similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix).flatten()\n",
        "\n",
        "    # Get top-ranked sentences (excluding the first one)\n",
        "    top_indices = similarity_scores.argsort()[-num_sentences:]\n",
        "    top_indices.sort()  # Preserve original order\n",
        "\n",
        "    # Combine top sentences\n",
        "    summary = ' '.join([sentences[i] for i in top_indices])\n",
        "    return summary\n",
        "\n",
        "# Main execution\n",
        "    # Replace this with the path to your PDF\n",
        "pdf_path = \"//22981A42B0_HARSHITH MUNAKALA (1) (2).pdf\"\n",
        "\n",
        "    # Step 1: Extract text\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Step 2: Summarize text\n",
        "summary = summarize_text(text, num_sentences=5)\n",
        "\n",
        "    # Step 3: Display summary\n",
        "print(\"\\n=== PDF Summary ===\\n\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z-0a4QLoDiG",
        "outputId": "cb4bfe0b-d284-45c8-88cd-6a7a3b36d41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PDF Summary ===\n",
            "\n",
            "HARSHITH  MUNAKALA  \n",
            " \n",
            "Visakhapatnam, Andhra Pradesh | harshaasharshith@gmail.com  |+91-\n",
            "9494444365|  www.linkedin.com/in/harshith -munakala -65149b31a  \n",
            " \n",
            "PROFESSIONAL SUMMARY  \n",
            " \n",
            "Motivated  undergraduate  pursuing  a Bachelor's  in Computer  Science  and Engineering  (CSE  \n",
            "- AI & ML). EDUCATION  \n",
            "Raghu  Engineering  College  2022 -Present  \n",
            "B.Tech  in Computer  Science  (AI & ML) \n",
            "Visakhapatnam, Andhra Pradesh  \n",
            "CGPA:  9.42 \n",
            "Sri Chaitanya  Junior  College  2020 -2022  \n",
            "Intermediate MPC \n",
            "Visakhapatnam,  Andhra Pradesh \n",
            "98% \n",
            "SKILLS  \n",
            "TECHNICAL  OTHERS  \n",
            "Front -End:  HTML, CSS. NPTEL  Certification  Course  in Programming  in C â€“ October  2023. GitHub: https://github.com/harshith -munakala/password_generator  \n",
            "2. GitHub: https://github.com/harshith -munakala/qr -code -generator\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "boILBpOGnTF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2 nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnXewtbymx7j",
        "outputId": "a59557ea-91ce-4a47-eee6-925bc4991887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvcevA02m9wA",
        "outputId": "71820cd2-d5a8-4d6f-b215-3f68c1f61d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12UoocArnKdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}